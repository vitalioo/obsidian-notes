# 5. Градиентный спуск и регуляризация L1 и L2

## Проблема переобучения и больших весов

Если просто минимизировать ошибку на обучающей выборке, модель:
- может подгоняться под шум
- раздувает веса
- плохо работает на новых данных

Логически:
> модель слишком сложная для данных

Чтобы это исправить, вводят **регуляризацию**.

---

## Идея регуляризации

Регуляризация — это штраф за сложность модели.
![[Pasted image 20260118194315.png]]
Мы минимизируем не только ошибку, но и величину весов:

Q(w) = ошибка + штраф за веса

---

## L2-регуляризация (Ridge)

L2 добавляет квадрат весов:

L2 = λ · Σ wᵢ²

Что это делает логически:
- большие веса становятся невыгодными
- модель предпочитает маленькие и «ровные» веса
- решение становится устойчивым

Градиент L2:
∂/∂wᵢ (λwᵢ²) = 2λwᵢ

То есть веса на каждом шаге немного «стягиваются к нулю».

L2:
- не зануляет веса
- уменьшает их величину
- хорошо работает при коллинеарности

---

## L1-регуляризация (Lasso)

L1 добавляет модуль весов:

L1 = λ · Σ |wᵢ|

Градиент:
- постоянный по модулю
- не зависит от величины веса

Логический эффект:
- маленькие веса быстро обнуляются
- модель отбрасывает лишние признаки

Итог:
- разреженные веса
- автоматический feature selection

---

## Геометрическая интерпретация

L2:
- круглая область допустимых весов
- минимум редко попадает точно на ось

L1:
- ромбовидная область
- углы лежат на осях
- отсюда зануление весов

---

## Регуляризация и аналитическое решение

Для L2 аналитическое решение становится:

ŵ = (XᵀX + λI)⁻¹ Xᵀy

Логический смысл:
- добавляем «устойчивость»
- матрица становится обратимой
- признаки меньше конфликтуют

---

## Итоговый смысл регуляризации

Регуляризация:
- ограничивает сложность модели
- борется с переобучением
- делает решение устойчивым

L2 — стабилизация  
L1 — отбор признаков
