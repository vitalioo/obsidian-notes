
6. Логистическая регрессия. Log Loss, margin, ...

логистическая регрессия = сигмоида от линейной регрессии  которая теперь выдает класс 1 или 0 но и вероятность быть классом 1 или 0
![[Pasted image 20260118184800.png]]
![[Pasted image 20260118184559.png]]
![[Pasted image 20260118190348.png]]![[Pasted image 20260118190622.png]]

# Билет 6. Логистическая регрессия. Log Loss, margin

## 1. Что такое логистическая регрессия

**Логистическая регрессия** — это модель **бинарной классификации**, которая отвечает на вопрос:

> *Какова вероятность того, что объект относится к положительному классу?*

Классы обычно кодируются как:
- `y = 1` — положительный класс  
- `y = 0` — отрицательный класс  

Модель **не просто говорит класс**, а сначала оценивает **вероятность** принадлежности к классу 1.

---

## 2. Линейная часть модели (margin)

Пусть у нас есть признаки объекта:

\[
x = (x_1, x_2, \dots, x_d)
\]

и веса модели:

\[
w = (w_1, w_2, \dots, w_d), \quad b \text{ — смещение}
\]

Сначала считается **линейная комбинация признаков**:

\[
z = w^\top x + b
\]

Эта величина называется **margin (отступ)**.

### Интуиция margin:
- `z > 0` → модель склоняется к классу 1  
- `z < 0` → модель склоняется к классу 0  
- `|z|` → уверенность модели  

Margin показывает:
> **насколько уверенно объект находится по одну из сторон разделяющей границы**

---

## 3. Почему нужна сигмоида

Значения `z` могут быть любыми: от `-∞` до `+∞`,  
а вероятность должна быть в диапазоне **[0, 1]**.

Поэтому применяется **сигмоидная функция**:

\[
\sigma(z) = \frac{1}{1 + e^{-z}}
\]

### Свойства сигмоиды:
- переводит любое число в интервал (0, 1)
- монотонно возрастает
- симметрична относительно точки (0, 0.5)

---

## 4. Вероятностная интерпретация

Выход логистической регрессии:

\[
p = P(y = 1 \mid x) = \sigma(w^\top x + b)
\]

Тогда:
- \(P(y=1) = p\)
- \(P(y=0) = 1 - p\)

Модель **не угадывает класс напрямую**, а говорит:
> *«С вероятностью 0.83 объект относится к классу 1»*

---

## 5. Как из вероятности получить класс

Обычно используется порог:

\[
\hat{y} =
\begin{cases}
1, & p \ge 0.5 \\
0, & p < 0.5
\end{cases}
\]

Но порог можно менять (например, при дисбалансе классов).

---

## 6. Функция потерь Log Loss (Binary Cross-Entropy)

Нам нужно обучить модель так, чтобы:
- высокая вероятность давалась **правильному классу**
- низкая — **неправильному**

Используется **логистическая функция потерь (Log Loss)**:

\[
\mathcal{L}(y, p) = - \left( y \log p + (1 - y) \log (1 - p) \right)
\]

---

## 7. Почему именно Log Loss

### Разбор по случаям

#### Если истинный класс `y = 1`:
\[
\mathcal{L} = -\log(p)
\]

- \(p \to 1\) → loss → 0 (хорошо)
- \(p \to 0\) → loss → ∞ (очень плохо)

#### Если истинный класс `y = 0`:
\[
\mathcal{L} = -\log(1 - p)
\]

- \(p \to 0\) → loss → 0
- \(p \to 1\) → loss → ∞

**Вывод:**  
Log Loss:
- сильно наказывает **уверенные ошибки**
- мягко относится к неуверенности

---

## 8. Связь Log Loss и правдоподобия

Log Loss получается из **максимизации правдоподобия**:

\[
\mathcal{L} = - \log P(y \mid x, w)
\]

То есть:
- мы подбираем веса так, чтобы **наблюдаемые классы были максимально вероятны**
- логарифм нужен для удобства оптимизации (сумма вместо произведения)

---

## 9. Обучение модели

Итоговая функция ошибки по выборке:

\[
J(w, b) = \frac{1}{n} \sum_{i=1}^{n} \mathcal{L}(y_i, \sigma(w^\top x_i + b))
\]

Минимизируется с помощью:
- градиентного спуска
- его улучшений (SGD, momentum, Adam и т.д.)

---

## 10. Геометрическая интерпретация

- Логистическая регрессия строит **линейную разделяющую гиперплоскость**
- Сигмоида «размазывает» жёсткую границу
- Margin отвечает за положение относительно границы
- Log Loss заставляет правильные точки быть **далеко от границы**

---

## 11. Плюсы и минусы логистической регрессии

### Плюсы:
- вероятностный вывод
- интерпретируемые коэффициенты
- быстрая и устойчивая
- хорошая база для понимания ML

### Минусы:
- линейная граница
- плохо работает на сложных нелинейных зависимостях
- чувствительна к выбросам без регуляризации

---

## 12. Краткий итог

- **Margin** — линейная уверенность модели  
- **Sigmoid** — перевод margin в вероятность  
- **Log Loss** — функция ошибки, штрафующая уверенные ошибки  
- **Логистическая регрессия** — вероятностная линейная модель бинарной классификации  

> Логистическая регрессия — это линейный классификатор, обучаемый через максимизацию правдоподобия с логарифмической функцией потерь.
