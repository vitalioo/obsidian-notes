
Bias Variance
Не всегда одиночные модели хороши и могут справиться
![[Pasted image 20260118230200.png]]
![[Pasted image 20260118230242.png]]
Bias (отклонение) 
- Смещение показывает насколько близки ответы модели к истине
- Разница между средним значением предсказания нашей модели и истинным значением. Чем больше отклонение тем сильнее модель упрощает данные -> ошибки на трейне
Variance (разброс) 
- дисперсия ответов наших моделей
- Вариативность значений предсказаний модели. Модель обращает больше внимания на данные чем это стоило бы делать, обращает внимание на шумы и помехи

Чем больше вариативность тем лучше отрабатывает на трейне но хуже на тесте

1. Big bias tiny variance - недообучились: ![[Pasted image 20260119141150.png]]
2. Big bias big variance - промежуточный шаг обучения, когда модель подбирает коэффициенты весов, поэтому и разброс ![[Pasted image 20260119141220.png]]
3. tiny bias tiny variance - отлично обучились! ![[Pasted image 20260119141418.png]]
4. tiny bias big variance - модель переобучилась, на результаты предсказания влияют помехи. Начала принимать во внимание те признаки на которые не стоит обращать внимание ![[Pasted image 20260119141455.png]]
Tradeoff:
![[Pasted image 20260119141709.png]]
Чем больше параметров тем меньше отклонение, однако тяжелее обобщение не обращая внимания на шумы и на помехи на признаки с меньшим весом и ценностью и тут же растёт Variance, модель хорошо отрабатывает на обучающем, но на тестовых наборах будет хуже

## Билет 9. Bias–Variance и разложение ошибки

### 1) С чего начать ответ: “из чего вообще состоит ошибка модели”

Когда мы оцениваем качество модели **на новых данных**, итоговая ошибка складывается из **трёх частей**:

1. **Шум (noise)** — “неустранимая” ошибка данных. Это то, что останется даже у _самой лучшей_ возможной модели на этой задаче: выбросы, случайные факторы, измерительные погрешности. С шумом “ничего не сделать”, можно только принять.
    
    BAGGING,_СЛУЧАЙНЫЙ_ЛЕС,_РАЗЛО…
    
2. **Смещение (bias)** — насколько модель _в среднем_ попадает в истину (насколько её “средний ответ” близок к истинной зависимости).
    
    BAGGING,_СЛУЧАЙНЫЙ_ЛЕС,_РАЗЛО…
    
3. **Разброс (variance)** — насколько сильно меняются ответы модели, если чуть поменять обучающую выборку (нестабильность модели).
    
    BAGGING,_СЛУЧАЙНЫЙ_ЛЕС,_РАЗЛО…
    

### 2) Интуитивные определения (это любят на экзамене)

- **Высокий bias**: модель слишком “простая”, она не умеет ловить сложную зависимость → **недообучение (underfitting)**.
    
- **Высокий variance**: модель слишком “чуткая” к данным, на другой выборке строит совсем другое решение → **переобучение (overfitting)**.
    
- Идеал: **и bias низкий, и variance низкий**, но на практике это компромисс.
    

### 3) Пример из лекции: дерево vs линейная модель

В лекции объясняют на синтетическом примере “истинного закона” (зелёная линия) и разных подвыборок с шумом:

BAGGING,_СЛУЧАЙНЫЙ_ЛЕС,_РАЗЛО…

**Дерево решений**:

- На разных подвыборках деревья дают очень разные “красные ломаные” → это **высокий variance** (нестабильность).
    
    BAGGING,_СЛУЧАЙНЫЙ_ЛЕС,_РАЗЛО…
    
- Но дерево способно приблизиться к сложной истинной зависимости → **низкий bias** (модель достаточно сложная).
    
    BAGGING,_СЛУЧАЙНЫЙ_ЛЕС,_РАЗЛО…
    

**Линейная регрессия**:

- На разных подвыборках прямые почти одинаковые → **низкий variance** (стабильная).
    
    BAGGING,_СЛУЧАЙНЫЙ_ЛЕС,_РАЗЛО…
    
- Но прямая не может повторить изгибы истинного закона → **высокий bias**.
    
    BAGGING,_СЛУЧАЙНЫЙ_ЛЕС,_РАЗЛО…
    

### 4) Важный вывод для ансамблей (ключ к следующему билету)

В лекции подчёркивают:

- **Смещение (bias) в ансамблях почти “не улучшается” само по себе**: если базовая модель уже “слабая” (высокий bias), ансамбль из неё не станет резко умнее.
    
    BAGGING,_СЛУЧАЙНЫЙ_ЛЕС,_РАЗЛО…
    
- Зато **разброс (variance) ансамбль умеет снижать**, потому что мы усредняем/голосуем по нескольким моделям.
    
    BAGGING,_СЛУЧАЙНЫЙ_ЛЕС,_РАЗЛО…
    

Отсюда правило: **лучшие кандидаты для bagging/random forest — модели с низким bias и высоким variance**, типично это **деревья решений**.