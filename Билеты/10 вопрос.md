Bagging, Random Forest

Из исходной выборки создаем подвыборки - за него отвечает bootstrap. Задаем разные алгоритмы после чего усредняем ответы
![[Pasted image 20260119152246.png]]
![[Pasted image 20260119152119.png]]
![[Pasted image 20260119152438.png]]
![[Pasted image 20260119154306.png]]
Просто берем считаем среднеквадратичную ошибку по всей выборке одной модели, усредняем ошибку по всем моделям. Предполагаем что все ошибки независимы между собой и строим новую модель: усредненное предсказание всех моделей
![[Pasted image 20260119155033.png]]
Bagging — это метод уменьшения разброса модели за счёт усреднения предсказаний нескольких моделей. Если ошибки отдельных моделей случайны и не зависят друг от друга, то при усреднении эти ошибки частично компенсируются, и итоговая модель становится более стабильной и точной.
![[Pasted image 20260119161034.png]]
![[Pasted image 20260119162052.png]]

> Random Forest — это ансамблевый метод, основанный на бэггинге деревьев решений с дополнительной случайностью по признакам. Для каждого дерева используется bootstrap-выборка объектов, а при каждом разбиении рассматривается случайное подмножество признаков. Это снижает корреляцию между деревьями и, как следствие, уменьшает variance и эффект переобучения, не увеличивая bias.
![[Pasted image 20260119163631.png]]

![[Pasted image 20260119184457.png]]
**Random Forest** — это ансамблевый метод машинного обучения, основанный на бэггинге деревьев решений с дополнительной случайностью по признакам.

В основе метода лежит идея обучения **нескольких деревьев решений на разных подвыборках данных**. Эти подвыборки формируются с помощью bootstrap: из исходного датасета случайно выбираются объекты с возвращением. В результате каждое дерево обучается на немного разных данных и получается нестабильным по-своему, что важно для снижения разброса.

Однако одного бэггинга недостаточно, потому что деревья всё равно могут получаться слишком похожими — особенно в верхних узлах, где часто выбираются одни и те же сильные признаки. Чтобы **повысить некоррелированность деревьев**, в Random Forest вводится дополнительная случайность: **при построении каждого узла дерева поиск лучшего разбиения ведётся не по всем признакам, а только по случайному подмножеству признаков**. Это заставляет разные деревья использовать разные признаки и формировать разные структуры.

Итоговое предсказание ансамбля получается путём агрегации предсказаний отдельных деревьев:

- **в задаче регрессии** берётся **среднее** предсказаний,
    
- **в задаче классификации** используется **голосование большинством**.
    

Основные гиперпараметры Random Forest — это **количество деревьев** и **гиперпараметры отдельного дерева** (глубина, минимальное число объектов в листе, критерий разбиения и т.д.). При увеличении числа деревьев качество обычно стабилизируется, а переобучение не усиливается.

Благодаря усреднению большого числа некоррелированных деревьев Random Forest обладает рядом преимуществ. Он показывает **высокую точность**, **менее чувствителен к выбросам**, **менее склонен к переобучению**, хорошо **параллелится**, так как деревья обучаются независимо, и часто **хорошо работает уже с дефолтными параметрами**, что делает его удобным базовым методом.

При этом у Random Forest есть и недостатки. Он **сложно интерпретируем**, так как состоит из большого числа деревьев. Он **плохо работает с разреженными признаками**, **не способен к экстраполяции** за пределы обучающих данных и требует **значительных затрат памяти**, поскольку хранит множество моделей одновременно.