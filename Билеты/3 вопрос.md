3. Ошибки для регрессии. Регрессия - аналитическое решение и градиентный спуск

У нас есть точки на плоскости: для каждого объекта известны
- **x** — признак (например, “площадь квартиры”)
- **y** — то, что хотим предсказать (например, “цена”)
На графиках это синие точки: каждая точка — один объект.
Мы хотим построить **модель**, которая по новому `x` сможет выдавать прогноз `y`.

![[Pasted image 20260118153256.png]]
если найти экстремум аналитически не получилось (решив полученные уравнения) то применяем градиентный спуск
![[Pasted image 20260118160536.png]]
![[Pasted image 20260118162052.png]]
# # 3. Ошибки для регрессии. Регрессия: аналитическое решение и градиентный спуск

## Что такое регрессия и зачем вообще нужна ошибка

Задача регрессии — это задача предсказания **числового значения**.  
Например: цена квартиры, температура, время в пути, стоимость товара.

У нас есть:
- реальные значения из данных: y
- предсказания модели: ŷ

Модель почти никогда не угадывает идеально, поэтому возникает **ошибка**.  
Ошибка — это способ **количественно измерить**, насколько модель ошибается.

Чтобы обучать модель, нам нужно:
1. Формально определить, что мы считаем «плохим предсказанием»
2. Минимизировать эту величину

Эта формальная мера и называется **функцией ошибки (loss function)**.

---

## Основные ошибки в регрессии

### Mean Squared Error (MSE)

MSE определяется как средний квадрат отклонений предсказаний от истинных значений:

MSE = (1 / N) * Σ (yᵢ − ŷᵢ)²

Логический смысл:
- если ошибка маленькая → квадрат ещё меньше
- если ошибка большая → квадрат резко возрастает

То есть MSE **сильно наказывает большие ошибки**.

Почему MSE так часто используют:
- функция гладкая
- имеет производные
- удобно оптимизировать с помощью градиентного спуска
- позволяет получить аналитическое решение

Недостаток:
- очень чувствительна к выбросам

---

### Mean Absolute Error (MAE)

MAE определяется как среднее абсолютное отклонение:

MAE = (1 / N) * Σ |yᵢ − ŷᵢ|

Логический смысл:
- каждая ошибка влияет линейно
- выбросы не доминируют

Плюс:
- устойчивость к выбросам

Минус:
- функция недифференцируема в нуле
- сложнее оптимизировать аналитически

---

## Линейная регрессия как модель

В простейшем виде линейная регрессия записывается так:

ŷ = w₁x₁ + w₂x₂ + ... + wₖxₖ + w₀

Где:
- x — признаки
- w — веса модели
- w₀ — свободный член (bias)

В матричном виде:

ŷ = Xw

Задача обучения модели:
> подобрать такие веса w, чтобы ошибка между y и ŷ была минимальной

---

## Аналитическое решение (метод наименьших квадратов)

Если в качестве ошибки используется MSE, можно получить **точную формулу для оптимальных весов**:

ŵ = (XᵀX)⁻¹ Xᵀy

Это и называется **аналитическим решением**.

Логический смысл:
- мы решаем систему линейных уравнений
- ищем точку минимума ошибки сразу, без итераций

Когда аналитическое решение применимо:
- признаки линейно независимы
- матрица XᵀX обратима
- данных не слишком много

Проблемы аналитического решения:
1. XᵀX может быть необратима (коллинеарные признаки)
2. При большом количестве данных вычисления очень дорогие
3. Решение нестабильно при шуме

Из-за этих проблем на практике часто используют **градиентный спуск**.

---

## Градиентный спуск как численный метод

Градиентный спуск — это итеративный метод оптимизации.

Идея:
- рассматриваем ошибку как функцию от весов
- идём в сторону, где ошибка уменьшается быстрее всего

Обновление весов:

w := w − α · ∇Q(w)

Где:
- ∇Q(w) — градиент функции ошибки
- α — learning rate (шаг)

Логически:
- градиент указывает направление роста ошибки
- мы идём в противоположную сторону
