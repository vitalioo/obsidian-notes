7. Оценка качества модели. Confusion matrix, Accuracy, Precision/Recall, F-score, ...
Accuracy - доля правильных ответов
![[Pasted image 20260118202521.png]]
false positive - ложное срабатывание. должна была не выдать кредит классу 0 но выдала 1 
false negative -  выдала класс 0 хотя клиенты могли оплатить (класс 1)
true positive - на 1 класс (выдать кредит можно) отвечает 1 классом
true negative - на 0 класс отвечает 0
Матрица ошибок = Confusion matrix
![[Pasted image 20260118203340.png]]
alpha(x) - ответ алгоритма
![[Pasted image 20260118203713.png]]
Presicion(точность) - доля истинных срабатываний от общего количества срабатываний
Recall(полнота) - доля обьектов относящихся к классу 1 которые алгоритм отнес к этому классу
значения метрик от 0 до 1
![[Pasted image 20260118204224.png]] 

![[Pasted image 20260118205208.png]]![[Pasted image 20260118210734.png]]
![[Pasted image 20260118210918.png]]![[Pasted image 20260118211227.png]]![[Pasted image 20260118211246.png]]
![[Pasted image 20260118211958.png]]
![[Pasted image 20260118212431.png]]
![[Pasted image 20260118212512.png]]
- **AUC** — чтобы **сравнить модели**
- **F-score** — чтобы **настроить порог и принимать решения**
> AUC оценивает способность модели ранжировать объекты независимо от порога, тогда как F-score оценивает качество классификации при фиксированном пороге.

# Билет 7. Оценка качества модели классификации

## 1. Зачем нужны метрики качества

Метрики качества используются для оценки того,  
насколько хорошо модель классификации:

- предсказывает классы,
    
- какие ошибки она совершает,
    
- и насколько эти ошибки критичны для задачи.
    

Одна метрика редко бывает достаточной,  
так как разные ошибки могут иметь разную цену.

---

## 2. Confusion Matrix (матрица ошибок)

Матрица ошибок показывает распределение предсказаний модели.

||Истинный 1|Истинный 0|
|---|---|---|
|Предсказан 1|TP|FP|
|Предсказан 0|FN|TN|

**Обозначения:**

- **TP (True Positive)** — верно предсказанный класс 1
    
- **FP (False Positive)** — ошибочно предсказан 1 вместо 0
    
- **FN (False Negative)** — ошибочно предсказан 0 вместо 1
    
- **TN (True Negative)** — верно предсказанный класс 0
    

Матрица ошибок является базой для всех остальных метрик.

---

## 3. Accuracy

**Accuracy** — доля правильных предсказаний:

`Accuracy = (TP + TN) / (TP + FP + FN + TN)`

### Интерпретация

- показывает, как часто модель угадывает класс.
    

### Недостатки

- плохо подходит для несбалансированных выборок;
    
- может быть высокой даже у бесполезной модели.
    

---

## 4. Precision и Recall

Эти метрики учитывают типы ошибок.

### Precision (точность)

`Precision = TP / (TP + FP)`

Показывает:

> какую долю действительно положительных объектов  
> составляют все объекты, предсказанные как положительные.

Используется, когда **ложные срабатывания дороги**.

---

### Recall (полнота, TPR)

`Recall = TP / (TP + FN)`

Показывает:

> какую долю всех положительных объектов  
> модель смогла обнаружить.

Используется, когда **пропуски положительных объектов критичны**.

---

## 5. F-score

Precision и Recall часто находятся в конфликте.  
**F-score** объединяет их в одну метрику.

### F1-score

`F1 = 2 * Precision * Recall / (Precision + Recall)`

**Свойства:**

- высок только при высоких Precision и Recall;
    
- сильно падает, если один из показателей мал.
    

Используется при несбалансированных классах  
и когда обе ошибки важны.

---

## 6. ROC-кривая

ROC-кривая показывает зависимость между:

- **TPR (True Positive Rate)** — Recall,
    
- **FPR (False Positive Rate)** = FP / (FP + TN).
    

Каждая точка ROC-кривой соответствует своему **порогу классификации**.

**Интерпретация:**

- диагональ — случайный классификатор;
    
- чем выше кривая над диагональю, тем лучше модель.
    

---

## 7. AUC (Area Under Curve)

**AUC** — площадь под ROC-кривой.

### Интерпретация

> AUC — вероятность того, что модель присвоит  
> более высокий скор положительному объекту,  
> чем отрицательному.

**Значения:**

- 0.5 — случайная модель;
    
- 0.7–0.8 — приемлемое качество;
    
- 0.8–0.9 — хорошая модель;
    
- > 0.9 — отличная модель.
    

---

## 8. Когда какую метрику использовать

- **Accuracy** — при сбалансированных классах;
    
- **Precision / Recall** — при неравной цене ошибок;
    
- **F1-score** — для баланса precision и recall;
    
- **ROC / AUC** — для сравнения моделей без выбора порога.
    

---

## 9. Итог

Для корректной оценки классификатора необходимо использовать  
несколько метрик, так как каждая из них отражает  
разные аспекты качества модели.