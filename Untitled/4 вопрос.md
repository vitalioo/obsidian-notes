# 4. Градиентный спуск и улучшения: momentum, batch

## Интуиция градиентного спуска

Функцию ошибки можно представить как поверхность:
- низины — хорошие значения весов
- вершины — плохие

Градиент в точке показывает:
> куда функция растёт быстрее всего

Чтобы минимизировать ошибку, мы:
> двигаемся в противоположную сторону

Отсюда правило обновления весов:
w := w − α · gradient

---

## Batch, Stochastic и Mini-batch Gradient Descent

### Batch Gradient Descent
- градиент считается по всем данным
- стабильный
- медленный

### Stochastic Gradient Descent (SGD)
- градиент считается по одному объекту
- быстрый
- шумный
- траектория движения зигзагообразная

### Mini-batch Gradient Descent
- градиент считается по небольшому батчу
- баланс скорости и стабильности
- используется чаще всего

---

## Проблема зигзагообразных колебаний

При SGD градиенты шумные:
- разные объекты тянут веса в разные стороны
- движение напоминает хаотичные скачки
- модель долго сходится

Эта проблема особенно заметна:
- в узких долинах функции ошибки
- при сильной разнице масштабов признаков

---

## Momentum: идея и логика

Momentum вводит **память о прошлом движении**.

Вводится вспомогательная переменная v — скорость:

vₜ = β · vₜ₋₁ + α · ∇Q(w)
w := w − vₜ

Где:
- β ∈ (0, 1) — коэффициент инерции

Логический смысл:
- v накапливает направление движения
- шумные колебания взаимно гасятся
- устойчивое направление усиливается

---

## Почему momentum уменьшает зигзаги

Без momentum:
- каждый шаг зависит только от текущего градиента
- направление постоянно меняется

С momentum:
- если градиенты меняют знак → они усредняются
- если градиенты совпадают → движение ускоряется

Итог:
- меньше колебаний
- быстрее движение к минимуму
- возможен «перепрыг» через локальные минимумы

Momentum можно интерпретировать как:
> физическое движение с инерцией
